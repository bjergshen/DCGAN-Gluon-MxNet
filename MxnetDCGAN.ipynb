{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN implemented on MxNet Gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import mxnet as mx\n",
    "import random\n",
    "from mxnet import nd, autograd\n",
    "from mxnet import gluon\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Options:\n",
    "    def __init__(self):\n",
    "        self.dataset = 'cifar' # cifar10 | lsun | imagenet | folder | lfw\n",
    "        #self.dataroot = '/EBS100G/GAN_resutls/lfw/lfw-deepfunneled' # path to dataset\n",
    "        self.dataroot = './data' # path to dataset\n",
    "        self.workers = 2 # number of data loading workers\n",
    "        self.batchSize = 64 # input batch size\n",
    "        self.imageSize = 64 # the height / width of the input image to network'\n",
    "        self.nz =100 # size of the latent z vector\n",
    "        self.ngf = 64\n",
    "        self.ndf = 64 \n",
    "        self.nc = 3 #numb color\n",
    "        self.niter =125 # number of epochs to train for\n",
    "        self.lr = 0.01 # learning rate, default=0.0002\n",
    "        self.beta1 = 0.5 # beta1 for adam. default=0.5\n",
    "        self.ctx = mx.gpu() #  enables gpu\n",
    "        self.ngpu = 1 # number of GPUs to use\n",
    "        self.G_net = '' # path to netG (to continue training)\n",
    "        self.D_net = '' #help=\"path to netD (to continue training)\")\n",
    "        self.outf = './data/mxnet_cifar' # help='folder to output images and model checkpoints')\n",
    "        self.manualSeed = random.randint(1, 10000) # manual seed \n",
    "\n",
    "opt = Options()\n",
    "try:\n",
    "    os.makedirs(opt.outf)\n",
    "except OSError:\n",
    "    pass\n",
    "mx.random.seed(opt.manualSeed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transformer(data, label):\n",
    "    data = mx.image.imresize(data, opt.imageSize, opt.imageSize)\n",
    "    data = mx.nd.transpose(data, (2,0,1))\n",
    "    data = data.astype(np.float32)/128-1\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creat a data iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR10(opt.dataroot, train=True, transform=transformer),\n",
    "    batch_size= opt.batchSize, shuffle=True, last_batch='discard')\n",
    "\n",
    "test_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR10(opt.dataroot, train=False, transform=transformer),\n",
    "    batch_size=opt.batchSize, shuffle=False, last_batch='discard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Creat the Generator and Discriminator networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G_Net = gluon.nn.Sequential()\n",
    "with G_Net.name_scope():\n",
    "    #first layer\n",
    "    G_Net.add(gluon.nn.Conv2DTranspose(channels=opt.ngf * 8, kernel_size=4,use_bias=False))\n",
    "    G_Net.add(gluon.nn.BatchNorm(axis = 1, momentum = 0.1,center=True))\n",
    "    G_Net.add(gluon.nn.Activation(\"relu\"))\n",
    "    #second layer\n",
    "    G_Net.add(gluon.nn.Conv2DTranspose(channels=opt.ngf * 4, kernel_size=4,strides = 2,padding=1,use_bias=False))\n",
    "    G_Net.add(gluon.nn.BatchNorm(axis = 1, momentum = 0.1,center=True))\n",
    "    G_Net.add(gluon.nn.Activation(\"relu\"))\n",
    "    #tird layer\n",
    "    G_Net.add(gluon.nn.Conv2DTranspose(channels=opt.ngf * 2, kernel_size=4,strides = 2,padding=1,use_bias=False))\n",
    "    G_Net.add(gluon.nn.BatchNorm(axis = 1, momentum = 0.1,center=True))\n",
    "    G_Net.add(gluon.nn.Activation(\"relu\"))\n",
    "    #fourth layer\n",
    "    G_Net.add(gluon.nn.Conv2DTranspose(channels=opt.ngf, kernel_size=4,strides = 2,padding=1,use_bias=False))\n",
    "    G_Net.add(gluon.nn.BatchNorm(axis = 1, momentum = 0.1,center=True))\n",
    "    G_Net.add(gluon.nn.Activation(\"relu\"))\n",
    "    #fifth layer\n",
    "    G_Net.add(gluon.nn.Conv2DTranspose(channels=opt.nc, kernel_size=4,strides = 2,padding=1,use_bias=False))\n",
    "    G_Net.add(gluon.nn.Activation(\"tanh\"))\n",
    "\n",
    "    \n",
    "    \n",
    "D_Net = gluon.nn.Sequential()\n",
    "with D_Net.name_scope():\n",
    "    #first layer\n",
    "    D_Net.add(gluon.nn.Conv2D(channels=opt.ndf , kernel_size=4,strides = 2,padding=1, use_bias=False))\n",
    "    D_Net.add(gluon.nn.LeakyReLU(0.2))\n",
    "    #second layer\n",
    "    D_Net.add(gluon.nn.Conv2D(channels=opt.ndf * 2, kernel_size=4,strides = 2,padding=1,use_bias=False))\n",
    "    D_Net.add(gluon.nn.BatchNorm(axis = 1, momentum = 0.1,center=True))\n",
    "    D_Net.add(gluon.nn.LeakyReLU(0.2))\n",
    "    #tird layer\n",
    "    D_Net.add(gluon.nn.Conv2D(channels=opt.ndf * 4, kernel_size=4,strides = 2,padding=1,use_bias=False))\n",
    "    D_Net.add(gluon.nn.BatchNorm(axis = 1, momentum = 0.1,center=True))\n",
    "    D_Net.add(gluon.nn.LeakyReLU(0.2))\n",
    "    #fourth layer\n",
    "    D_Net.add(gluon.nn.Conv2D(channels=opt.ndf * 8, kernel_size=4,strides = 2,padding=1,use_bias=False))\n",
    "    D_Net.add(gluon.nn.BatchNorm(axis = 1, momentum = 0.1,center=True))\n",
    "    D_Net.add(gluon.nn.LeakyReLU(0.2))\n",
    "    #fifth layer\n",
    "    D_Net.add(gluon.nn.Conv2D(channels=opt.ndf * 8, kernel_size=4,strides = 2,padding=0,use_bias=False))\n",
    "    D_Net.add(gluon.nn.Dense(1,activation ='sigmoid'))\n",
    "    \n",
    "g_net = G_Net\n",
    "d_net = D_Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A function to visualizing the created and real images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def image_show(data,padding = 2):\n",
    "    import math\n",
    "    datanp = (data.asnumpy().transpose((0, 2, 3, 1))+1.0)*128\n",
    "    x_dim = min(8, opt.batchSize)\n",
    "    y_dim = int(math.ceil(float(opt.batchSize) / x_dim))\n",
    "    height, width = int(opt.imageSize + padding), int(opt.imageSize + padding)\n",
    "    grid = np.zeros(( height * y_dim + 1 + padding // 2, width * x_dim + 1 + padding // 2,3))\n",
    "    k = 0\n",
    "    for y in range(y_dim):\n",
    "        for x in range(x_dim):\n",
    "            if k >= opt.batchSize:\n",
    "                break\n",
    "            start_y = y * height + 1 + padding // 2\n",
    "            end_y = start_y + height - padding \n",
    "            start_x = x * width + 1 + padding // 2\n",
    "            end_x = start_x + width - padding\n",
    "            np.copyto(grid[start_y:end_y,start_x:end_x],datanp[k])\n",
    "            k = k + 1\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(grid)\n",
    "    plt.show()\n",
    "def binary_cross_entropy(yhat, y):\n",
    "    return - (y * nd.log(yhat)+(1 - y ) * nd.log(1 - yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the networks and the optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialization\n",
    "\n",
    "g_net.collect_params().initialize(mx.init.Xavier(magnitude=0.02), ctx=opt.ctx)\n",
    "d_net.collect_params().initialize(mx.init.Xavier(magnitude=0.02), ctx=opt.ctx)\n",
    "G_trainer = gluon.Trainer(g_net.collect_params(), 'Adam', {'learning_rate': opt.lr * 100,'beta1':opt.beta1})\n",
    "D_trainer = gluon.Trainer(d_net.collect_params(), 'Adam', {'learning_rate': opt.lr,'beta1':opt.beta1})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The learning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_net.collect_params().zero_grad()\n",
    "d_net.collect_params().zero_grad()\n",
    "counter = 0\n",
    "for epoch in range(opt.niter):\n",
    "    for i, (d, _ ) in enumerate(train_data):\n",
    "        # update D\n",
    "        data = d.as_in_context(opt.ctx)\n",
    "        label = nd.ones(opt.batchSize, opt.ctx)\n",
    "        with autograd.record():            \n",
    "            output = d_net(data)\n",
    "            D_error = nd.mean(binary_cross_entropy(output,label))\n",
    "            D_x = nd.mean(output)\n",
    "\n",
    "        D_error.backward()\n",
    "        D_trainer.step(data.shape[0])\n",
    "        \n",
    "        noise = mx.ndarray.normal(loc = 0, scale = 1, shape = (opt.batchSize, opt.nz,1,1),ctx = opt.ctx)\n",
    "        fake_image = g_net(noise)\n",
    "        label = nd.zeros(opt.batchSize, opt.ctx)\n",
    "\n",
    "        with autograd.record():\n",
    "            output = d_net(fake_image)\n",
    "            D_error_fake_image = nd.mean(binary_cross_entropy(output,label))\n",
    "            D_G_z1 = nd.mean(output)\n",
    "\n",
    "        D_error_fake_image.backward()\n",
    "        D_trainer.step(fake_image.shape[0])\n",
    "            \n",
    "        #update G\n",
    "        label = nd.ones(opt.batchSize, opt.ctx)\n",
    "        with autograd.record():\n",
    "            fake_image = g_net(noise)\n",
    "            print(fake_image)\n",
    "            output = d_net(fake_image)\n",
    "            G_error = nd.mean(binary_cross_entropy(output,label))\n",
    "            D_G_z2 = nd.mean(output)\n",
    "        G_error.backward()\n",
    "        G_trainer.step(fake_image.shape[0])\n",
    "        \n",
    "        \n",
    "        print('[%d/%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "              % (epoch,counter, opt.niter, i, len(train_data),\n",
    "                 D_error.asnumpy(), G_error.asnumpy(), D_x.asnumpy(), D_G_z1.asnumpy(), D_G_z2.asnumpy()))\n",
    "        trunctate = 1\n",
    "        if i % (10*trunctate) == 0:\n",
    "            #image_show(data)\n",
    "            image_show(fake_image)\n",
    "        if i % (100*trunctate) == 0:   \n",
    "            filenameG = '%s/%s/G_Net_epoch_%d_%d' % (opt.outf, opt.dataset, epoch,counter)\n",
    "            filenameD = '%s/%s/D_Net_epoch_%d_%d' % (opt.outf, opt.dataset, epoch,counter)\n",
    "            g_net.save_params(filenameG)\n",
    "            d_net.save_params(filenameD)\n",
    "            counter = counter + 1\n",
    "    counter = 0\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
